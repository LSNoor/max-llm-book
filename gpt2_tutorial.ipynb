{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VSxvy8s5yPsN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCQGyliFxq4U"
      },
      "source": [
        "# Building GPT-2 from Scratch with MAX\n",
        "\n",
        "This interactive notebook guides you through building GPT-2 step by step. Each section corresponds to a step in the tutorial.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's verify MAX is installed and import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre modular \\\n",
        "  --index-url https://dl.modular.com/public/nightly/python/simple/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJG78Y16xzb7",
        "outputId": "7e4c8487-dda8-443a-e6d1-ed8bbf5579db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://dl.modular.com/public/nightly/python/simple/\n",
            "Requirement already satisfied: modular in /usr/local/lib/python3.12/dist-packages (25.7.0.dev2025110305)\n",
            "Requirement already satisfied: max==25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (25.7.0.dev2025110305)\n",
            "Requirement already satisfied: mojo==0.25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from modular) (0.25.7.0.dev2025110305)\n",
            "Requirement already satisfied: max-core==25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from max==25.7.0.dev2025110305->max[benchmark]==25.7.0.dev2025110305->modular) (25.7.0.dev2025110305)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from max==25.7.0.dev2025110305->max[benchmark]==25.7.0.dev2025110305->modular) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from max==25.7.0.dev2025110305->max[benchmark]==25.7.0.dev2025110305->modular) (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from max==25.7.0.dev2025110305->max[benchmark]==25.7.0.dev2025110305->modular) (6.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (8.3.0)\n",
            "Requirement already satisfied: gguf>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.17.1)\n",
            "Requirement already satisfied: hf-transfer>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.1.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.36.0)\n",
            "Requirement already satisfied: jinja2>=3.1.6 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (3.1.6)\n",
            "Requirement already satisfied: llguidance>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (1.3.0)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (11.3.0)\n",
            "Requirement already satisfied: psutil>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (7.1.3)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.0.1 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.2.1)\n",
            "Requirement already satisfied: taskgroup>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.2.2)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (4.57.1)\n",
            "Requirement already satisfied: uvicorn>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.38.0)\n",
            "Requirement already satisfied: uvloop>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.22.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.5 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (3.13.1)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (4.0.0)\n",
            "Requirement already satisfied: msgspec>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.19.0)\n",
            "Requirement already satisfied: safetensors>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from max[benchmark]==25.7.0.dev2025110305->modular) (0.6.2)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (24.1.0)\n",
            "Requirement already satisfied: asgiref>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (3.10.0)\n",
            "Requirement already satisfied: fastapi>=0.115.3 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.120.1)\n",
            "Requirement already satisfied: grpcio>=1.68.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (1.76.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.28.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.29.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http>=1.27.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-prometheus>=0.50b0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.56b0)\n",
            "Requirement already satisfied: opentelemetry-sdk<1.36.0,>=1.29.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (1.35.0)\n",
            "Requirement already satisfied: prometheus-client>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.23.1)\n",
            "Requirement already satisfied: protobuf<6.32.0,>=6.31.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (6.31.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (2.11.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (2.11.10)\n",
            "Requirement already satisfied: pyinstrument>=5.0.1 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (5.1.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.7 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (4.0.0)\n",
            "Requirement already satisfied: pyzmq>=26.3.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (27.1.0)\n",
            "Requirement already satisfied: regex>=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (1.16.3)\n",
            "Requirement already satisfied: sse-starlette>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.47.2 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.49.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from max[serve]==25.7.0.dev2025110305->modular) (0.22.1)\n",
            "Requirement already satisfied: mojo-compiler==0.25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from mojo==0.25.7.0.dev2025110305->modular) (0.25.7.0.dev2025110305)\n",
            "Requirement already satisfied: mblack==25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from mojo==0.25.7.0.dev2025110305->modular) (25.7.0.dev2025110305)\n",
            "Requirement already satisfied: mojo-lldb-libs==0.25.7.0.dev2025110305 in /usr/local/lib/python3.12/dist-packages (from mojo==0.25.7.0.dev2025110305->modular) (0.25.7.0.dev2025110305)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from mblack==25.7.0.dev2025110305->mojo==0.25.7.0.dev2025110305->modular) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from mblack==25.7.0.dev2025110305->mojo==0.25.7.0.dev2025110305->modular) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from mblack==25.7.0.dev2025110305->mojo==0.25.7.0.dev2025110305->modular) (4.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.5->max[benchmark]==25.7.0.dev2025110305->modular) (1.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (25.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.115.3->max[serve]==25.7.0.dev2025110305->modular) (0.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.28.0->max[benchmark]==25.7.0.dev2025110305->modular) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.6->max[benchmark]==25.7.0.dev2025110305->modular) (3.0.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.29.0->max[serve]==25.7.0.dev2025110305->modular) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.7.0.dev2025110305->modular) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.7.0.dev2025110305->modular) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.27.0->max[serve]==25.7.0.dev2025110305->modular) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<1.36.0,>=1.29.0->max[serve]==25.7.0.dev2025110305->modular) (0.56b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.7.0.dev2025110305->modular) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.7.0.dev2025110305->modular) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->max[serve]==25.7.0.dev2025110305->modular) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.7.1->max[serve]==25.7.0.dev2025110305->modular) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->max[benchmark]==25.7.0.dev2025110305->modular) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.3->max[benchmark]==25.7.0.dev2025110305->modular) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.1->max[benchmark]==25.7.0.dev2025110305->modular) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.0.1->max[benchmark]==25.7.0.dev2025110305->modular) (2.19.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.12/dist-packages (from taskgroup>=0.2.2->max[benchmark]==25.7.0.dev2025110305->modular) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.28.1->max[serve]==25.7.0.dev2025110305->modular) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.29.0->max[serve]==25.7.0.dev2025110305->modular) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.0.1->max[benchmark]==25.7.0.dev2025110305->modular) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->max[benchmark]==25.7.0.dev2025110305->modular) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBmr261yxq4V"
      },
      "outputs": [],
      "source": [
        "# Run this cell to verify MAX is installed and import necessary libraries\n",
        "try:\n",
        "    from dataclasses import dataclass\n",
        "    import math\n",
        "    import numpy as np\n",
        "\n",
        "    from max.driver import CPU, Device\n",
        "    from max.dtype import DType\n",
        "    from max.experimental import functional as F\n",
        "    from max.experimental.tensor import Tensor\n",
        "    from max.graph import Dim, DimLike\n",
        "    from max.nn.module_v3 import Embedding, Linear, Module, Sequential\n",
        "\n",
        "    # Verify MAX is working by creating a simple tensor\n",
        "    test_tensor = Tensor.ones([2, 2], dtype=DType.float32, device=CPU())\n",
        "\n",
        "    print(\"✓ MAX is installed and working correctly!\")\n",
        "    print(f\"✓ All imports successful\")\n",
        "    print(f\"✓ Test tensor created: shape={test_tensor.shape}\")\n",
        "    print(\"\\nYou're ready to start building GPT-2!\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"❌ Error: MAX is not installed or not accessible\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    print(\"\\nTo fix this:\")\n",
        "    print(\"1. Install MAX: https://docs.modular.com/max/\")\n",
        "    print(\"2. Run 'pixi install' from the project root\")\n",
        "    print(\"3. Start Jupyter in the pixi environment: 'pixi run jupyter lab'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dugTNI1Txq4X"
      },
      "source": [
        "## Step 01: Model Configuration\n",
        "\n",
        "Before you can implement GPT-2, you need to define its architecture - the dimensions, layer counts, and structural parameters.\n",
        "\n",
        "Create `GPT2Config`, a class that holds all the architectural decisions for GPT-2.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Add the `@dataclass` decorator to the class\n",
        "2. Get the parameter values from the [GPT-2 config.json](https://huggingface.co/openai-community/gpt2/blob/main/config.json)\n",
        "3. Replace each `None` with the correct value:\n",
        "   - `vocab_size`: 50,257\n",
        "   - `n_positions`: 1,024\n",
        "   - `n_embd`: 768\n",
        "   - `n_layer`: 12\n",
        "   - `n_head`: 12\n",
        "   - `n_inner`: 3,072\n",
        "   - `layer_norm_epsilon`: 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R1C_Wgpxq4X"
      },
      "outputs": [],
      "source": [
        "# TODO: Add @dataclass decorator\n",
        "class GPT2Config:\n",
        "    vocab_size: int = None  # TODO: Replace with correct value\n",
        "    n_positions: int = None  # TODO: Replace with correct value\n",
        "    n_embd: int = None  # TODO: Replace with correct value\n",
        "    n_layer: int = None  # TODO: Replace with correct value\n",
        "    n_head: int = None  # TODO: Replace with correct value\n",
        "    n_inner: int = None  # TODO: Replace with correct value\n",
        "    layer_norm_epsilon: float = None  # TODO: Replace with correct value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPKDrUFpxq4X"
      },
      "outputs": [],
      "source": [
        "# Validation cell - run this to check your configuration\n",
        "config = GPT2Config()\n",
        "assert config.vocab_size == 50257, f\"Expected vocab_size=50257, got {config.vocab_size}\"\n",
        "assert config.n_positions == 1024, f\"Expected n_positions=1024, got {config.n_positions}\"\n",
        "assert config.n_embd == 768, f\"Expected n_embd=768, got {config.n_embd}\"\n",
        "assert config.n_layer == 12, f\"Expected n_layer=12, got {config.n_layer}\"\n",
        "assert config.n_head == 12, f\"Expected n_head=12, got {config.n_head}\"\n",
        "assert config.n_inner == 3072, f\"Expected n_inner=3072, got {config.n_inner}\"\n",
        "assert config.layer_norm_epsilon == 1e-5, f\"Expected layer_norm_epsilon=1e-5, got {config.layer_norm_epsilon}\"\n",
        "print(\"✓ Step 01 complete! Configuration is correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umdXbT7Uxq4X"
      },
      "source": [
        "## Step 02: Causal Masking\n",
        "\n",
        "Implement the `causal_mask()` function that prevents the model from \"seeing\" future tokens during autoregressive generation.\n",
        "\n",
        "The mask creates a lower triangular pattern where each token can only attend to itself and previous tokens.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Add `@F.functional` decorator to convert the function to a MAX graph operation\n",
        "2. Calculate total sequence length: `n = Dim(sequence_length) + num_tokens`\n",
        "3. Create a `-inf` constant: `Tensor.constant(float(\"-inf\"), dtype=dtype, device=device)`\n",
        "4. Broadcast to target shape: `F.broadcast_to(mask, shape=(sequence_length, n))`\n",
        "5. Apply band_part to create lower triangular pattern: `F.band_part(mask, num_lower=None, num_upper=0, exclude=True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovyw73XYxq4X"
      },
      "outputs": [],
      "source": [
        "# TODO: Add @F.functional decorator\n",
        "def causal_mask(\n",
        "    sequence_length: DimLike,\n",
        "    num_tokens: DimLike,\n",
        "    dtype: DType,\n",
        "    device: Device,\n",
        ") -> Tensor:\n",
        "    # TODO: Calculate total length n\n",
        "    n = None\n",
        "\n",
        "    # TODO: Create -inf constant tensor\n",
        "    mask = None\n",
        "\n",
        "    # TODO: Broadcast to (sequence_length, n) shape\n",
        "    mask = None\n",
        "\n",
        "    # TODO: Apply band_part to create lower triangular pattern\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNeRiehZxq4X"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "test_mask = causal_mask(3, 0, DType.float32, CPU())\n",
        "print(\"Causal mask for sequence length 3:\")\n",
        "print(test_mask)\n",
        "print(\"\\n✓ Step 02 complete! Causal mask is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k3zevIHxq4X"
      },
      "source": [
        "## Step 03: Layer Normalization\n",
        "\n",
        "Create the `LayerNorm` class that normalizes activations across the feature dimension to stabilize training.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Initialize `self.weight` using `Tensor.ones([dim])`\n",
        "2. Initialize `self.bias` using `Tensor.zeros([dim])`\n",
        "3. Apply layer normalization using `F.layer_norm(x, gamma=self.weight, beta=self.bias, epsilon=self.eps)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f8vAz5Xxq4Y"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        # TODO: Initialize weight parameter with ones\n",
        "        self.weight = None\n",
        "        # TODO: Initialize bias parameter with zeros\n",
        "        self.bias = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: Apply layer normalization\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9bBG7Guxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "ln = LayerNorm(768)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = ln(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 03 complete! Layer normalization is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TdqNKYPxq4Y"
      },
      "source": [
        "## Step 04: Feed-Forward Network (MLP)\n",
        "\n",
        "Build the `MLP` class, a two-layer feed-forward network with GELU activation.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create expansion layer: `Linear(embed_dim, intermediate_size, bias=True)` → `self.c_fc`\n",
        "2. Create projection layer: `Linear(intermediate_size, embed_dim, bias=True)` → `self.c_proj`\n",
        "3. In forward pass:\n",
        "   - Apply expansion: `self.c_fc(hidden_states)`\n",
        "   - Apply GELU: `F.gelu(hidden_states, approximate=\"tanh\")`\n",
        "   - Apply projection: `self.c_proj(hidden_states)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoSeKq46xq4Y"
      },
      "outputs": [],
      "source": [
        "class MLP(Module):\n",
        "    def __init__(self, embed_dim: int, intermediate_size: int):\n",
        "        super().__init__()\n",
        "        # TODO: Create expansion layer (c_fc)\n",
        "        self.c_fc = None\n",
        "        # TODO: Create projection layer (c_proj)\n",
        "        self.c_proj = None\n",
        "\n",
        "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
        "        # TODO: Apply expansion\n",
        "        hidden_states = None\n",
        "        # TODO: Apply GELU activation\n",
        "        hidden_states = None\n",
        "        # TODO: Apply projection and return\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsRVLSMjxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "mlp = MLP(768, 3072)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = mlp(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 04 complete! MLP is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1OrolaOxq4Y"
      },
      "source": [
        "## Step 05: Token Embeddings\n",
        "\n",
        "Create the token embedding layer that converts discrete token IDs into continuous vector representations.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create token embedding layer: `Embedding(config.vocab_size, dim=config.n_embd)` → `self.wte`\n",
        "2. In forward pass: `self.wte(input_ids)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfRF1j9Uxq4Y"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create token embedding layer (wte)\n",
        "        self.wte = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Lookup token embeddings\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "837rgHcsxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "token_emb = TokenEmbedding(config)\n",
        "test_tokens = Tensor([1, 2, 3, 4, 5], dtype=DType.int64, device=CPU())\n",
        "output = token_emb(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output embeddings shape: {output.shape}\")\n",
        "print(\"✓ Step 05 complete! Token embeddings working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-7nfhBaxq4Y"
      },
      "source": [
        "## Step 06: Position Embeddings\n",
        "\n",
        "Create position embeddings to encode where each token appears in the sequence.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create position embedding layer: `Embedding(config.n_positions, dim=config.n_embd)` → `self.wpe`\n",
        "2. In forward pass: `self.wpe(position_ids)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgQyOc8zxq4Y"
      },
      "outputs": [],
      "source": [
        "class PositionEmbedding(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create position embedding layer (wpe)\n",
        "        self.wpe = None\n",
        "\n",
        "    def forward(self, position_ids: Tensor) -> Tensor:\n",
        "        # TODO: Lookup position embeddings\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFjAnaCcxq4Y"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "pos_emb = PositionEmbedding(config)\n",
        "test_positions = Tensor.arange(5, dtype=DType.int64, device=CPU())\n",
        "output = pos_emb(test_positions)\n",
        "print(f\"Input positions shape: {test_positions.shape}\")\n",
        "print(f\"Output embeddings shape: {output.shape}\")\n",
        "print(\"✓ Step 06 complete! Position embeddings working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-XN9ilSxq4Y"
      },
      "source": [
        "## Step 07: Q/K/V Projections (Single Head)\n",
        "\n",
        "Implement Q/K/V projections for attention. GPT-2 uses a combined projection layer for efficiency.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create combined Q/K/V projection: `Linear(config.n_embd, 3 * config.n_embd, bias=True)` → `self.c_attn`\n",
        "2. In forward pass:\n",
        "   - Apply projection: `qkv = self.c_attn(x)`\n",
        "   - Split into Q, K, V: `F.split(qkv, [self.n_embd, self.n_embd, self.n_embd], axis=-1)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNedEeoGxq4Y"
      },
      "outputs": [],
      "source": [
        "class QKVProjection(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        # TODO: Create combined Q/K/V projection layer (c_attn)\n",
        "        self.c_attn = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        # TODO: Apply combined projection\n",
        "        qkv = None\n",
        "        # TODO: Split into Q, K, V\n",
        "        query, key, value = None, None, None\n",
        "        return query, key, value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaZiXjbcxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "qkv_proj = QKVProjection(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "q, k, v = qkv_proj(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Query shape: {q.shape}\")\n",
        "print(f\"Key shape: {k.shape}\")\n",
        "print(f\"Value shape: {v.shape}\")\n",
        "print(\"✓ Step 07 complete! Q/K/V projections working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXqcBt7kxq4Z"
      },
      "source": [
        "## Step 08: Attention Mechanism with Causal Masking\n",
        "\n",
        "Implement the core attention mechanism using scaled dot-product attention.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Compute attention scores: `query @ key.transpose(-1, -2)`\n",
        "2. Scale scores: divide by `math.sqrt(int(value.shape[-1]))`\n",
        "3. Apply causal mask: `attn_weights + causal_mask(...)`\n",
        "4. Apply softmax: `F.softmax(attn_weights)`\n",
        "5. Compute output: `attn_weights @ value`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22UNTEmZxq4Z"
      },
      "outputs": [],
      "source": [
        "def compute_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
        "    # TODO: Compute attention scores (Q @ K^T)\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Scale by sqrt(d_k)\n",
        "    scale_factor = None\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Apply causal mask\n",
        "    seq_len = query.shape[-2]\n",
        "    mask = causal_mask(seq_len, 0, dtype=query.dtype, device=query.device)\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Apply softmax\n",
        "    attn_weights = None\n",
        "\n",
        "    # TODO: Compute weighted sum of values\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS2I_zrDxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "test_q = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "test_k = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "test_v = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = compute_attention(test_q, test_k, test_v)\n",
        "print(f\"Query shape: {test_q.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 08 complete! Attention mechanism working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQMFe5vExq4Z"
      },
      "source": [
        "## Step 09: Multi-Head Attention\n",
        "\n",
        "Extend single-head attention to multi-head attention by splitting embeddings across 12 heads.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create Q/K/V projection: `Linear(config.n_embd, 3 * config.n_embd, bias=True)` → `self.c_attn`\n",
        "2. Create output projection: `Linear(config.n_embd, config.n_embd, bias=True)` → `self.c_proj`\n",
        "3. Implement `_split_heads`: reshape and transpose to `[batch, num_heads, seq_length, head_dim]`\n",
        "4. Implement `_merge_heads`: reverse the splitting operation\n",
        "5. In forward: project → split heads → attention → merge heads → output projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU2XiVx9xq4Z"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.n_head\n",
        "        self.head_dim = config.n_embd // config.n_head\n",
        "        self.split_size = config.n_embd\n",
        "\n",
        "        # TODO: Create combined Q/K/V projection\n",
        "        self.c_attn = None\n",
        "        # TODO: Create output projection\n",
        "        self.c_proj = None\n",
        "\n",
        "    def _split_heads(self, tensor: Tensor, num_heads: int, attn_head_size: int) -> Tensor:\n",
        "        # TODO: Reshape to add head dimension\n",
        "        new_shape = tensor.shape[:-1] + [num_heads, attn_head_size]\n",
        "        tensor = None\n",
        "        # TODO: Transpose to move heads to position 1\n",
        "        return None\n",
        "\n",
        "    def _merge_heads(self, tensor: Tensor, num_heads: int, attn_head_size: int) -> Tensor:\n",
        "        # TODO: Transpose heads back\n",
        "        tensor = None\n",
        "        # TODO: Reshape to flatten heads\n",
        "        new_shape = tensor.shape[:-2] + [num_heads * attn_head_size]\n",
        "        return None\n",
        "\n",
        "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
        "        # TODO: Project to Q/K/V and split\n",
        "        qkv = None\n",
        "        query, key, value = None, None, None\n",
        "\n",
        "        # TODO: Split heads for Q, K, V\n",
        "        query = None\n",
        "        key = None\n",
        "        value = None\n",
        "\n",
        "        # TODO: Compute attention\n",
        "        attn_output = None\n",
        "\n",
        "        # TODO: Merge heads\n",
        "        attn_output = None\n",
        "\n",
        "        # TODO: Apply output projection\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCxvQg1Mxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "mha = MultiHeadAttention(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = mha(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 09 complete! Multi-head attention working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdk9SUXxq4Z"
      },
      "source": [
        "## Step 10: Residual Connections and Layer Normalization Pattern\n",
        "\n",
        "The LayerNorm class is already implemented in Step 03. This step demonstrates the pre-norm pattern used in GPT-2.\n",
        "\n",
        "The pattern is: `x = x + sublayer(layer_norm(x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fHtKsN7xq4Z"
      },
      "outputs": [],
      "source": [
        "# Demonstration of pre-norm pattern\n",
        "def apply_with_residual(x: Tensor, layer_norm: LayerNorm, sublayer: Module) -> Tensor:\n",
        "    \"\"\"Apply pre-norm pattern: x = x + sublayer(layer_norm(x))\"\"\"\n",
        "    return x + sublayer(layer_norm(x))\n",
        "\n",
        "# Validation\n",
        "ln = LayerNorm(768)\n",
        "mlp_layer = MLP(768, 3072)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = apply_with_residual(test_input, ln, mlp_layer)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 10 complete! Pre-norm pattern demonstrated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRz6soQhxq4Z"
      },
      "source": [
        "## Step 11: Transformer Block\n",
        "\n",
        "Combine attention, MLP, layer normalization, and residual connections into a complete transformer block.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create first layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` → `self.ln_1`\n",
        "2. Create attention: `MultiHeadAttention(config)` → `self.attn`\n",
        "3. Create second layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` → `self.ln_2`\n",
        "4. Create MLP: `MLP(config.n_embd, config.n_inner)` → `self.mlp`\n",
        "5. In forward:\n",
        "   - `x = x + self.attn(self.ln_1(x))`\n",
        "   - `x = x + self.mlp(self.ln_2(x))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lp8zS2Qxq4Z"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create first layer norm (ln_1)\n",
        "        self.ln_1 = None\n",
        "        # TODO: Create multi-head attention (attn)\n",
        "        self.attn = None\n",
        "        # TODO: Create second layer norm (ln_2)\n",
        "        self.ln_2 = None\n",
        "        # TODO: Create MLP (mlp)\n",
        "        self.mlp = None\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # TODO: Apply attention with pre-norm and residual\n",
        "        x = None\n",
        "        # TODO: Apply MLP with pre-norm and residual\n",
        "        x = None\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmYZgvYJxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "block = TransformerBlock(config)\n",
        "test_input = Tensor.randn([1, 10, 768], dtype=DType.float32, device=CPU())\n",
        "output = block(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"✓ Step 11 complete! Transformer block working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWH6jzc3xq4Z"
      },
      "source": [
        "## Step 12: Stacking Transformer Blocks\n",
        "\n",
        "Stack 12 transformer blocks with embeddings and final normalization to create the complete GPT-2 model.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create token embeddings: `Embedding(config.vocab_size, dim=config.n_embd)` → `self.wte`\n",
        "2. Create position embeddings: `Embedding(config.n_positions, dim=config.n_embd)` → `self.wpe`\n",
        "3. Stack transformer blocks: `Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])` → `self.h`\n",
        "4. Create final layer norm: `LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)` → `self.ln_f`\n",
        "5. In forward:\n",
        "   - Get sequence length and create positions\n",
        "   - Combine token and position embeddings\n",
        "   - Pass through transformer blocks\n",
        "   - Apply final layer norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYtVFS_mxq4Z"
      },
      "outputs": [],
      "source": [
        "class GPT2Model(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create token embeddings (wte)\n",
        "        self.wte = None\n",
        "        # TODO: Create position embeddings (wpe)\n",
        "        self.wpe = None\n",
        "        # TODO: Stack transformer blocks (h)\n",
        "        self.h = None\n",
        "        # TODO: Create final layer norm (ln_f)\n",
        "        self.ln_f = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Get sequence length\n",
        "        seq_length = None\n",
        "\n",
        "        # TODO: Create position indices\n",
        "        positions = None\n",
        "\n",
        "        # TODO: Get token and position embeddings\n",
        "        token_embeds = None\n",
        "        pos_embeds = None\n",
        "\n",
        "        # TODO: Combine embeddings\n",
        "        hidden_states = None\n",
        "\n",
        "        # TODO: Pass through transformer blocks\n",
        "        hidden_states = None\n",
        "\n",
        "        # TODO: Apply final layer norm\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y7_IWgAxq4Z"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "model = GPT2Model(config)\n",
        "test_tokens = Tensor([[1, 2, 3, 4, 5]], dtype=DType.int64, device=CPU())\n",
        "output = model(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output hidden states shape: {output.shape}\")\n",
        "print(\"✓ Step 12 complete! GPT-2 model working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24TpwEJvxq4Z"
      },
      "source": [
        "## Step 13: Language Model Head\n",
        "\n",
        "Add the final linear projection layer that converts hidden states to vocabulary logits.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Create transformer: `GPT2Model(config)` → `self.transformer`\n",
        "2. Create LM head: `Linear(config.n_embd, config.vocab_size, bias=False)` → `self.lm_head`\n",
        "3. In forward:\n",
        "   - Get hidden states from transformer\n",
        "   - Project to logits with LM head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Futk0uPKxq4i"
      },
      "outputs": [],
      "source": [
        "class GPT2LMHeadModel(Module):\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "        # TODO: Create transformer\n",
        "        self.transformer = None\n",
        "        # TODO: Create LM head (note: bias=False)\n",
        "        self.lm_head = None\n",
        "\n",
        "    def forward(self, input_ids: Tensor) -> Tensor:\n",
        "        # TODO: Get hidden states from transformer\n",
        "        hidden_states = None\n",
        "        # TODO: Project to vocabulary logits\n",
        "        logits = None\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUzuuyJtxq4i"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "lm_model = GPT2LMHeadModel(config)\n",
        "test_tokens = Tensor([[1, 2, 3, 4, 5]], dtype=DType.int64, device=CPU())\n",
        "logits = lm_model(test_tokens)\n",
        "print(f\"Input tokens shape: {test_tokens.shape}\")\n",
        "print(f\"Output logits shape: {logits.shape}\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")\n",
        "print(\"✓ Step 13 complete! Language model head working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMQORLmBxq4i"
      },
      "source": [
        "## Step 14: Text Generation\n",
        "\n",
        "Implement text generation with temperature and sampling.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "1. Implement `generate_next_token`:\n",
        "   - Get logits from model\n",
        "   - Extract last position logits\n",
        "   - Apply temperature scaling\n",
        "   - Sample from probability distribution or use greedy decoding\n",
        "2. Implement `generate`:\n",
        "   - Loop for max_new_tokens iterations\n",
        "   - Generate next token\n",
        "   - Concatenate to sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diEb8li7xq4i"
      },
      "outputs": [],
      "source": [
        "def generate_next_token(\n",
        "    model: GPT2LMHeadModel,\n",
        "    input_ids: Tensor,\n",
        "    temperature: float = 1.0,\n",
        "    do_sample: bool = True,\n",
        ") -> Tensor:\n",
        "    # TODO: Get logits from model\n",
        "    logits = None\n",
        "\n",
        "    # TODO: Extract last position logits\n",
        "    next_token_logits = None\n",
        "\n",
        "    if do_sample:\n",
        "        # TODO: Create temperature tensor\n",
        "        temp_tensor = None\n",
        "        # TODO: Scale logits by temperature\n",
        "        next_token_logits = None\n",
        "\n",
        "        # TODO: Convert to probabilities\n",
        "        probs = None\n",
        "\n",
        "        # TODO: Transfer to CPU and convert to NumPy\n",
        "        probs_np = None\n",
        "\n",
        "        # TODO: Sample from distribution\n",
        "        next_token_id = None\n",
        "\n",
        "        # TODO: Convert back to tensor\n",
        "        next_token_tensor = None\n",
        "    else:\n",
        "        # TODO: Greedy decoding (argmax)\n",
        "        next_token_tensor = None\n",
        "\n",
        "    return next_token_tensor\n",
        "\n",
        "def generate(\n",
        "    model: GPT2LMHeadModel,\n",
        "    input_ids: Tensor,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float = 1.0,\n",
        "    do_sample: bool = True,\n",
        ") -> Tensor:\n",
        "    # TODO: Initialize with input tokens\n",
        "    generated_tokens = input_ids\n",
        "\n",
        "    # TODO: Generate loop\n",
        "    for _ in range(max_new_tokens):\n",
        "        # TODO: Generate next token\n",
        "        next_token = None\n",
        "\n",
        "        # TODO: Reshape to 2D\n",
        "        next_token_2d = None\n",
        "\n",
        "        # TODO: Concatenate to sequence\n",
        "        generated_tokens = None\n",
        "\n",
        "    return generated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrhwvxBHxq4j"
      },
      "outputs": [],
      "source": [
        "# Validation cell\n",
        "# Note: This will generate random tokens since we haven't loaded pretrained weights\n",
        "prompt = Tensor([[15496, 995]], dtype=DType.int64, device=CPU())  # \"Hello world\"\n",
        "generated = generate(lm_model, prompt, max_new_tokens=5, temperature=1.0, do_sample=False)\n",
        "print(f\"Prompt shape: {prompt.shape}\")\n",
        "print(f\"Generated sequence shape: {generated.shape}\")\n",
        "print(f\"Generated token IDs: {generated}\")\n",
        "print(\"\\n✓ Step 14 complete! Text generation working.\")\n",
        "print(\"\\n🎉 Congratulations! You've built GPT-2 from scratch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tRs3EBsxq4j"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You've completed all 14 steps and built a complete GPT-2 model from scratch using MAX!\n",
        "\n",
        "### What you've built:\n",
        "\n",
        "1. **Model configuration** - Architecture parameters\n",
        "2. **Causal masking** - Prevents attending to future tokens\n",
        "3. **Layer normalization** - Stabilizes training\n",
        "4. **Feed-forward network (MLP)** - Adds non-linearity\n",
        "5. **Token embeddings** - Converts token IDs to vectors\n",
        "6. **Position embeddings** - Encodes sequence order\n",
        "7. **Q/K/V projections** - Prepares inputs for attention\n",
        "8. **Attention mechanism** - Core attention computation\n",
        "9. **Multi-head attention** - Parallel attention heads\n",
        "10. **Residual connections** - Enables deep networks\n",
        "11. **Transformer block** - Complete repeating unit\n",
        "12. **Model stacking** - Combines all components\n",
        "13. **Language model head** - Projects to vocabulary\n",
        "14. **Text generation** - Autoregressive sampling\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}